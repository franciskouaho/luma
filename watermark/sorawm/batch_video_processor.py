#!/usr/bin/env python3
"""
Batch Video Processor - Traitement de 20 vid√©os en parall√®le
Optimis√© pour traiter plusieurs vid√©os simultan√©ment avec gestion intelligente des ressources
"""

import json
import queue
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Callable, Dict, List, Optional

from loguru import logger
from tqdm import tqdm

from sorawm.ultra_fast_video import process_video_ultra_fast


class BatchVideoProcessor:
    """
    Processeur de vid√©os en batch ultra-rapide

    Capacit√©s:
    - 20 vid√©os en parall√®le
    - Gestion intelligente de la m√©moire
    - Priorisation des t√¢ches
    - Monitoring en temps r√©el
    """

    def __init__(
        self,
        max_concurrent_videos: int = 20,
        max_workers_per_video: int = 2,
        temp_base_dir: str = "tmp_batch_processing",
        progress_callback: Optional[Callable] = None,
    ):
        """
        Args:
            max_concurrent_videos: Nombre max de vid√©os trait√©es simultan√©ment
            max_workers_per_video: Workers par vid√©o (r√©duit pour √©viter surcharge)
            temp_base_dir: Dossier de base pour les fichiers temporaires
            progress_callback: Callback pour mise √† jour du progr√®s
        """
        self.max_concurrent = max_concurrent_videos
        self.workers_per_video = max_workers_per_video
        self.temp_base_dir = Path(temp_base_dir)
        self.temp_base_dir.mkdir(exist_ok=True, parents=True)
        self.progress_callback = progress_callback

        # File d'attente des t√¢ches
        self.task_queue = queue.Queue()
        self.completed_tasks = queue.Queue()

        # Statistiques globales
        self.stats = {
            "total_videos": 0,
            "completed_videos": 0,
            "failed_videos": 0,
            "total_processing_time": 0.0,
            "average_time_per_video": 0.0,
            "concurrent_peak": 0,
            "throughput_videos_per_minute": 0.0,
        }

        # Lock pour thread safety
        self.stats_lock = threading.Lock()

        logger.info(f"BatchVideoProcessor initialis√©:")
        logger.info(f"  üìπ Max vid√©os simultan√©es: {max_concurrent_videos}")
        logger.info(f"  ‚ö° Workers par vid√©o: {max_workers_per_video}")
        logger.info(f"  üìÅ Dossier temporaire: {temp_base_dir}")

    def add_video(
        self,
        input_path: str,
        output_path: str,
        priority: int = 1,
        metadata: Optional[Dict] = None,
    ) -> str:
        """
        Ajoute une vid√©o √† la file d'attente

        Args:
            input_path: Chemin vid√©o source
            output_path: Chemin vid√©o de sortie
            priority: Priorit√© (1=haute, 5=basse)
            metadata: M√©tadonn√©es additionnelles

        Returns:
            ID unique de la t√¢che
        """
        task_id = f"task_{int(time.time() * 1000)}_{len(self.task_queue.queue)}"

        task = {
            "id": task_id,
            "input_path": input_path,
            "output_path": output_path,
            "priority": priority,
            "metadata": metadata or {},
            "status": "QUEUED",
            "progress": 0,
            "start_time": None,
            "end_time": None,
            "error": None,
            "processing_time": 0.0,
        }

        self.task_queue.put((priority, task))

        with self.stats_lock:
            self.stats["total_videos"] += 1

        logger.info(
            f"‚ûï Vid√©o ajout√©e √† la queue: {Path(input_path).name} (ID: {task_id})"
        )
        return task_id

    def add_videos_bulk(self, video_pairs: List[tuple], priority: int = 1) -> List[str]:
        """
        Ajoute plusieurs vid√©os en une fois

        Args:
            video_pairs: Liste de tuples (input_path, output_path)
            priority: Priorit√© par d√©faut

        Returns:
            Liste des IDs des t√¢ches
        """
        task_ids = []
        for input_path, output_path in video_pairs:
            task_id = self.add_video(input_path, output_path, priority)
            task_ids.append(task_id)

        logger.info(f"üì¶ {len(video_pairs)} vid√©os ajout√©es au batch")
        return task_ids

    def _process_single_video(self, task: Dict) -> Dict:
        """
        Traite une seule vid√©o avec gestion d'erreurs

        Args:
            task: Dictionnaire de la t√¢che

        Returns:
            T√¢che mise √† jour avec r√©sultats
        """
        task_id = task["id"]
        input_path = task["input_path"]
        output_path = task["output_path"]

        try:
            task["status"] = "PROCESSING"
            task["start_time"] = time.time()

            logger.info(f"üé¨ D√©but traitement: {Path(input_path).name}")

            # Cr√©er dossier temporaire unique pour cette t√¢che
            temp_dir = self.temp_base_dir / f"task_{task_id}"
            temp_dir.mkdir(exist_ok=True, parents=True)

            # Traiter la vid√©o avec param√®tres optimis√©s
            result = process_video_ultra_fast(
                input_path=input_path,
                output_path=output_path,
                max_workers=self.workers_per_video,
                skip_frames=1,  # Toutes les frames pour qualit√© maximale
                max_resolution=1080,
                cleanup=True,
            )

            task["end_time"] = time.time()
            task["processing_time"] = task["end_time"] - task["start_time"]

            if result.get("success", False):
                task["status"] = "COMPLETED"
                task["progress"] = 100
                task["result"] = result

                with self.stats_lock:
                    self.stats["completed_videos"] += 1
                    self.stats["total_processing_time"] += task["processing_time"]

                logger.info(
                    f"‚úÖ Termin√©: {Path(input_path).name} en {task['processing_time']:.1f}s"
                )
            else:
                raise Exception(result.get("error", "Erreur inconnue"))

        except Exception as e:
            task["status"] = "FAILED"
            task["error"] = str(e)
            task["end_time"] = time.time()
            task["processing_time"] = (
                task["end_time"] - task["start_time"] if task["start_time"] else 0
            )

            with self.stats_lock:
                self.stats["failed_videos"] += 1

            logger.error(f"‚ùå √âchec: {Path(input_path).name} - {e}")

        finally:
            # Nettoyer le dossier temporaire
            temp_dir = self.temp_base_dir / f"task_{task_id}"
            if temp_dir.exists():
                import shutil

                shutil.rmtree(temp_dir, ignore_errors=True)

        return task

    def _update_progress(self, completed_tasks: List[Dict]):
        """Met √† jour les statistiques et appelle le callback de progr√®s"""
        if not self.progress_callback:
            return

        total = self.stats["total_videos"]
        completed = len(completed_tasks)

        if total > 0:
            progress_percent = (completed / total) * 100

            # Calculer ETA
            if completed > 0:
                avg_time = (
                    sum(t.get("processing_time", 0) for t in completed_tasks)
                    / completed
                )
                remaining_videos = total - completed
                eta_seconds = remaining_videos * avg_time / self.max_concurrent
            else:
                eta_seconds = 0

            self.progress_callback(
                {
                    "progress": progress_percent,
                    "completed": completed,
                    "total": total,
                    "failed": self.stats["failed_videos"],
                    "eta_seconds": eta_seconds,
                    "concurrent_active": min(self.max_concurrent, total - completed),
                }
            )

    def process_all(self) -> Dict:
        """
        Traite toutes les vid√©os en parall√®le

        Returns:
            Statistiques finales du traitement
        """
        if self.task_queue.empty():
            logger.warning("Aucune vid√©o dans la queue")
            return self.get_stats()

        start_time = time.time()
        logger.info(f"üöÄ D√âBUT TRAITEMENT BATCH: {self.stats['total_videos']} vid√©os")

        # Convertir la queue en liste tri√©e par priorit√©
        tasks = []
        while not self.task_queue.empty():
            priority, task = self.task_queue.get()
            tasks.append(task)

        # Trier par priorit√© (1 = haute priorit√©)
        tasks.sort(key=lambda x: x["priority"])

        completed_tasks = []
        active_tasks = []

        # Utiliser ThreadPoolExecutor pour g√©rer le parall√©lisme
        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            # Soumettre les premi√®res t√¢ches
            future_to_task = {}

            for i, task in enumerate(tasks):
                if len(future_to_task) < self.max_concurrent:
                    future = executor.submit(self._process_single_video, task)
                    future_to_task[future] = task
                    active_tasks.append(task)
                else:
                    break

            # Index pour les t√¢ches restantes
            next_task_index = len(future_to_task)

            # Traiter les r√©sultats au fur et √† mesure
            with tqdm(total=len(tasks), desc="üé¨ Traitement batch") as pbar:
                while future_to_task:
                    # Attendre qu'une t√¢che se termine
                    for future in as_completed(future_to_task):
                        completed_task = future.result()
                        completed_tasks.append(completed_task)

                        # Retirer de la liste active
                        if completed_task in active_tasks:
                            active_tasks.remove(completed_task)

                        # Mettre √† jour la barre de progression
                        pbar.update(1)
                        pbar.set_postfix(
                            {
                                "‚úÖ": len(
                                    [
                                        t
                                        for t in completed_tasks
                                        if t["status"] == "COMPLETED"
                                    ]
                                ),
                                "‚ùå": len(
                                    [
                                        t
                                        for t in completed_tasks
                                        if t["status"] == "FAILED"
                                    ]
                                ),
                                "üîÑ": len(active_tasks),
                            }
                        )

                        # Supprimer cette future
                        del future_to_task[future]

                        # Ajouter une nouvelle t√¢che si il en reste
                        if next_task_index < len(tasks):
                            next_task = tasks[next_task_index]
                            new_future = executor.submit(
                                self._process_single_video, next_task
                            )
                            future_to_task[new_future] = next_task
                            active_tasks.append(next_task)
                            next_task_index += 1

                        # Mettre √† jour le progr√®s
                        self._update_progress(completed_tasks)

                        # Une seule it√©ration par future compl√©t√©e
                        break

        # Calcul des statistiques finales
        total_time = time.time() - start_time
        successful_tasks = [t for t in completed_tasks if t["status"] == "COMPLETED"]
        failed_tasks = [t for t in completed_tasks if t["status"] == "FAILED"]

        with self.stats_lock:
            self.stats["total_processing_time"] = total_time
            if len(successful_tasks) > 0:
                self.stats["average_time_per_video"] = sum(
                    t["processing_time"] for t in successful_tasks
                ) / len(successful_tasks)
            self.stats["throughput_videos_per_minute"] = (
                len(successful_tasks) / total_time
            ) * 60
            self.stats["concurrent_peak"] = min(self.max_concurrent, len(tasks))

        # Rapport final
        logger.info("üéØ TRAITEMENT BATCH TERMIN√â:")
        logger.info(f"  ‚è±Ô∏è  Temps total: {total_time:.1f}s")
        logger.info(f"  ‚úÖ Succ√®s: {len(successful_tasks)}/{len(tasks)}")
        logger.info(f"  ‚ùå √âchecs: {len(failed_tasks)}")
        logger.info(
            f"  üìä D√©bit: {self.stats['throughput_videos_per_minute']:.1f} vid√©os/minute"
        )
        logger.info(
            f"  ‚ö° Temps moyen par vid√©o: {self.stats['average_time_per_video']:.1f}s"
        )

        if failed_tasks:
            logger.warning("‚ùå √âCHECS D√âTECT√âS:")
            for task in failed_tasks:
                logger.warning(f"  - {Path(task['input_path']).name}: {task['error']}")

        return {
            "success": len(failed_tasks) == 0,
            "completed_tasks": successful_tasks,
            "failed_tasks": failed_tasks,
            "stats": self.get_stats(),
        }

    def get_stats(self) -> Dict:
        """Retourne les statistiques actuelles"""
        with self.stats_lock:
            return self.stats.copy()

    def save_results(self, output_path: str, results: Dict):
        """Sauvegarde les r√©sultats du batch dans un fichier JSON"""
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, default=str)
        logger.info(f"üìÑ R√©sultats sauv√©s: {output_path}")


def process_videos_batch(
    video_pairs: List[tuple],
    max_concurrent: int = 20,
    workers_per_video: int = 2,
    progress_callback: Optional[Callable] = None,
) -> Dict:
    """
    Fonction utilitaire pour traiter un batch de vid√©os

    Args:
        video_pairs: Liste de tuples (input_path, output_path)
        max_concurrent: Nombre max de vid√©os simultan√©es
        workers_per_video: Workers par vid√©o
        progress_callback: Callback de progr√®s

    Returns:
        R√©sultats du traitement
    """
    processor = BatchVideoProcessor(
        max_concurrent_videos=max_concurrent,
        max_workers_per_video=workers_per_video,
        progress_callback=progress_callback,
    )

    # Ajouter toutes les vid√©os
    processor.add_videos_bulk(video_pairs)

    # Traiter le batch
    return processor.process_all()


if __name__ == "__main__":
    """Test du processeur batch"""
    import argparse

    parser = argparse.ArgumentParser(description="Batch Video Watermark Remover")
    parser.add_argument("--input-dir", required=True, help="Dossier des vid√©os sources")
    parser.add_argument("--output-dir", required=True, help="Dossier de sortie")
    parser.add_argument("--concurrent", type=int, default=20, help="Vid√©os simultan√©es")
    parser.add_argument("--workers", type=int, default=2, help="Workers par vid√©o")
    parser.add_argument("--pattern", default="*.mp4", help="Pattern des fichiers")

    args = parser.parse_args()

    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Trouver toutes les vid√©os
    video_files = list(input_dir.glob(args.pattern))
    if not video_files:
        logger.error(
            f"Aucune vid√©o trouv√©e dans {input_dir} avec pattern {args.pattern}"
        )
        exit(1)

    # Cr√©er les paires input/output
    video_pairs = []
    for video_file in video_files:
        output_file = output_dir / f"cleaned_{video_file.name}"
        video_pairs.append((str(video_file), str(output_file)))

    logger.info(f"üé¨ Traitement batch: {len(video_pairs)} vid√©os")
    logger.info(f"üìÅ Source: {input_dir}")
    logger.info(f"üìÅ Destination: {output_dir}")

    # Callback de progr√®s simple
    def progress_callback(info):
        logger.info(
            f"üìä Progr√®s: {info['progress']:.1f}% ({info['completed']}/{info['total']}) - "
            f"ETA: {info['eta_seconds']:.0f}s - Actifs: {info['concurrent_active']}"
        )

    # Traiter le batch
    start_time = time.time()
    results = process_videos_batch(
        video_pairs,
        max_concurrent=args.concurrent,
        workers_per_video=args.workers,
        progress_callback=progress_callback,
    )

    total_time = time.time() - start_time

    # Sauvegarder les r√©sultats
    results_file = output_dir / "batch_results.json"
    processor = BatchVideoProcessor()
    processor.save_results(str(results_file), results)

    # Rapport final
    successful = len(results["completed_tasks"])
    failed = len(results["failed_tasks"])
    total = successful + failed

    print(f"\nüéØ RAPPORT FINAL:")
    print(f"‚è±Ô∏è  Temps total: {total_time:.1f}s")
    print(f"‚úÖ Succ√®s: {successful}/{total} ({successful / total * 100:.1f}%)")
    print(f"‚ùå √âchecs: {failed}")
    print(
        f"üìä D√©bit: {results['stats']['throughput_videos_per_minute']:.1f} vid√©os/minute"
    )

    if successful > 0:
        print(f"üöÄ PERFORMANCE: {20} vid√©os trait√©es simultan√©ment!")
        print(
            f"‚ö° Temps moyen: {results['stats']['average_time_per_video']:.1f}s par vid√©o"
        )
